{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-september",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WbDje0GMPMG",
    "outputId": "57cdcc28-eec2-4a65-e2b7-bc5bbd0ddfa4"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-lottery",
   "metadata": {
    "id": "handy-convert"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import metrics, optimizers, losses\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import f1_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-fundamental",
   "metadata": {
    "id": "spatial-gross"
   },
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-roberts",
   "metadata": {
    "id": "unlike-nightlife"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/ner_dataset.csv', sep=\",\", encoding=\"latin1\").fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-annex",
   "metadata": {
    "id": "express-interval"
   },
   "outputs": [],
   "source": [
    "class ContextNER:\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.__df = df\n",
    "\n",
    "        self.all_words = set(df.Word.values)\n",
    "        self.all_tags = set(df.Tag.values)\n",
    "\n",
    "        self.num_words = len(self.all_words)\n",
    "        self.num_tags = len(self.all_tags) + 1\n",
    "\n",
    "        self.sentences = self.__build_sentences()\n",
    "        self.max_len = self.__get_maxlen()\n",
    "\n",
    "        self.__build_Xy()\n",
    "        self.__build_parsers()\n",
    "\n",
    "    def __get_maxlen(self):\n",
    "        return max([len(x) for x in self.sentences]) \n",
    "\n",
    "    def __build_sentences(self):\n",
    "\n",
    "        return [x for x in self.__df.groupby('Sentence #').apply(\n",
    "            lambda xdef: [x for x in zip(\n",
    "                xdef.Word.values,\n",
    "                xdef.Tag.values\n",
    "            )]\n",
    "        )]\n",
    "\n",
    "    def __build_Xy(self):\n",
    "\n",
    "        self.X = [[word for word, __ in value] for value in self.sentences]\n",
    "        self.y = [[tag for __, tag in value] for value in self.sentences]\n",
    "\n",
    "    def __build_parsers(self):\n",
    "\n",
    "        self.word2idx = {value: idx for idx,\n",
    "                         value in enumerate(self.all_words)}\n",
    "\n",
    "        # Converte um index em Word\n",
    "        self.idx2word = {idx: value for value, idx in self.word2idx.items()}\n",
    "\n",
    "        # Converte Tag em Ã¬ndice\n",
    "        self.tag2idx = {value: idx + 1 for idx,\n",
    "                        value in enumerate(self.all_tags)}\n",
    "        self.tag2idx[\"[PAD]\"] = 0  # Padding - Preenchimento\n",
    "\n",
    "        # Converte index em Tag\n",
    "        self.idx2tag = {idx: value for value, idx in self.tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-recommendation",
   "metadata": {
    "id": "amateur-recommendation"
   },
   "outputs": [],
   "source": [
    "contextNER = ContextNER(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-wright",
   "metadata": {
    "id": "canadian-senior"
   },
   "outputs": [],
   "source": [
    "Words, Tags = contextNER.X, contextNER.y\n",
    "\n",
    "max_seq_length = contextNER.max_len\n",
    "pad_token_label_id = 0\n",
    "special_tokens_count =  2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-latter",
   "metadata": {
    "id": "concerned-lounge"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, \n",
    "                                        do_lower_case=False)\n",
    "\n",
    "def convert_to_input(sentences, tags):\n",
    "\n",
    "    input_id_list, attention_mask_list, token_type_id_list = [], [], []\n",
    "    label_id_list = []\n",
    "  \n",
    "    for x, y in tqdm(zip(sentences, tags), total = len(tags)):\n",
    "  \n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "\n",
    "        for word, label in zip(x, y):\n",
    "            \n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            label_ids.extend([contextNER.tag2idx[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "            \n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "        label_ids = [pad_token_label_id] + label_ids + [pad_token_label_id]\n",
    "        inputs = tokenizer.encode_plus(tokens,\n",
    "                                       add_special_tokens=True, \n",
    "                                       truncation=True,\n",
    "                                       max_length=max_seq_length)\n",
    "\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "        attention_masks = [1] * len(input_ids)\n",
    "\n",
    "        attention_mask_list.append(attention_masks)\n",
    "        input_id_list.append(input_ids)\n",
    "        token_type_id_list.append(token_type_ids)\n",
    "\n",
    "        label_id_list.append(label_ids)\n",
    "\n",
    "    return input_id_list, token_type_id_list, attention_mask_list, label_id_list\n",
    "\n",
    "\n",
    "def pad_seq(seq, max_seq_length):\n",
    "    return pad_sequences(seq,\n",
    "                         maxlen=max_seq_length,\n",
    "                         dtype=\"long\",\n",
    "                         truncating=\"post\",\n",
    "                         padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-tokyo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "divided-racing",
    "outputId": "6f5835e5-0755-49fc-bafa-646fada7acc7"
   },
   "outputs": [],
   "source": [
    "input_ids_train, token_ids_train, attention_masks_train, label_ids_train = convert_to_input(Words, Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-horror",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "responsible-devil",
    "outputId": "73f5b2e3-bee0-4297-c5e8-e2bda3c15f28"
   },
   "outputs": [],
   "source": [
    "for token_id, tag_id in zip(input_ids_train[0], label_ids_train[0]):\n",
    "    \n",
    "    word = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    tag = contextNER.idx2tag[tag_id]\n",
    "    \n",
    "    print(token_id, ' - ', word, ' - ', tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-chile",
   "metadata": {
    "id": "visible-maple"
   },
   "outputs": [],
   "source": [
    "input_ids_train = pad_seq(input_ids_train, max_seq_length)\n",
    "token_ids_train = pad_seq(token_ids_train, max_seq_length)\n",
    "attention_masks_train = pad_seq(attention_masks_train, max_seq_length)\n",
    "label_ids_train = pad_seq(label_ids_train, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-minimum",
   "metadata": {
    "id": "persistent-harmony"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-night",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "furnished-depth",
    "outputId": "a09be8f9-d945-410b-bcd3-66880da391f2"
   },
   "outputs": [],
   "source": [
    "input_ids = layers.Input(shape=(max_seq_length,), \n",
    "                         dtype=tf.int32, \n",
    "                         name=\"input_ids\")\n",
    "\n",
    "token_type_ids = layers.Input(shape=(max_seq_length,), \n",
    "                              dtype=tf.int32, \n",
    "                              name=\"attention_masks\")\n",
    "\n",
    "attention_masks = layers.Input(shape=(max_seq_length,), \n",
    "                               dtype=tf.int32,\n",
    "                               name=\"token_type_ids\")\n",
    "\n",
    "bert_inputs = [input_ids, token_type_ids, attention_masks]\n",
    "\n",
    "bert_configs = BertConfig.from_pretrained(BERT_MODEL_NAME, num_labels=contextNER.num_tags)\n",
    "bert_model = TFBertModel.from_pretrained(BERT_MODEL_NAME, config=bert_configs)\n",
    "bert_model.trainable = False\n",
    "\n",
    "sequence_output = bert_model(bert_inputs)[0]\n",
    "\n",
    "# Recebe os embedings/features da camada pre-treinada anterior (BERT)\n",
    "\n",
    "# bi_lstm = layers.Bidirectional(layers.LSTM(max_seq_length // 2, \n",
    "#                                            return_sequences=True,\n",
    "#                                            recurrent_dropout=0.1), name='bilstm')(sequence_output)\n",
    "\n",
    "# Usar com GPU para acelerar treinamento\n",
    "bi_lstm = layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(max_seq_length // 2, \n",
    "                                                             return_sequences=True),\n",
    "                                                             name='bilstm')(sequence_output)\n",
    "\n",
    "dropout = layers.TimeDistributed(layers.Dropout(0.3))(bi_lstm)\n",
    "\n",
    "dense_layer = layers.TimeDistributed(layers.Dense(max_seq_length,\n",
    "                                                  activation='relu',\n",
    "                                                  name='last_dense'))(dropout)\n",
    "\n",
    "output = layers.Dense(contextNER.num_tags,\n",
    "                               activation=\"softmax\",\n",
    "                               name='predictions')(dense_layer)\n",
    "\n",
    "model = models.Model(inputs=bert_inputs, outputs=output)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001),\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[metrics.SparseCategoricalAccuracy('accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-trading",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ethical-greene",
    "outputId": "c2005007-fc5e-4029-e062-61b61ac0eaea"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-hearing",
   "metadata": {
    "id": "critical-frederick"
   },
   "outputs": [],
   "source": [
    "x_train = [input_ids_train,\n",
    "           attention_masks_train,\n",
    "           token_ids_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-faculty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kd11u55YN7V5",
    "outputId": "b70eead2-45d7-49f1-cc82-23b8b7c11104"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=x_train,\n",
    "                    y=label_ids_train,\n",
    "                    validation_split=0.3, \n",
    "                    batch_size=16, \n",
    "                    epochs=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-college",
   "metadata": {
    "id": "t7pZOsgAPuZH"
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BerTokenizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
